{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfwwn+Y71KY/O6yEw/+53q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anuhya-web/DNN_final_template/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbB6m5qCAGwQ"
      },
      "outputs": [],
      "source": [
        "# @title Importing the necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from datasets.fingerprint import random\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import models\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as FT\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "import gc\n",
        "\n",
        "import textwrap\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PerceptualLoss, self).__init__()\n",
        "\n",
        "        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features\n",
        "\n",
        "        self.layers = nn.Sequential(*list(vgg[:16])).eval()\n",
        "\n",
        "        for p in self.layers.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        return nn.functional.l1_loss(self.layers(x), self.layers(y))"
      ],
      "metadata": {
        "id": "070ZXsLjAVwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def edge_loss(pred, target, device):\n",
        "    edge_x = torch.tensor([[1,0,-1],\n",
        "                            [2,0,-2],\n",
        "                            [1,0,-1]], dtype=torch.float32, device=device).view(1,1,3,3)\n",
        "\n",
        "    edge_y = edge_x.transpose(2,3)\n",
        "\n",
        "    def apply(x):\n",
        "        ex = nn.functional.conv2d(x, edge_x, padding=1, groups=3)\n",
        "        ey = nn.functional.conv2d(x, edge_y, padding=1, groups=3)\n",
        "        return torch.sqrt(ex**2 + ey**2 + 1e-6)\n",
        "\n",
        "    return nn.functional.l1_loss(apply(pred), apply(target))\n"
      ],
      "metadata": {
        "id": "s141JOk7AibW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setting up google drive to save checkpoints\n",
        "\n",
        "# This will prompt you to authorize Google Drive access\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "def save_checkpoint_to_drive(model, optimizer, epoch, loss, filename=\"autoencoder_checkpoint.pth\"):\n",
        "    \"\"\"\n",
        "    Saves the checkpoint directly to a specified folder in your mounted Google Drive.\n",
        "    \"\"\"\n",
        "    # 1. Define the full Google Drive path\n",
        "    # 'DL_Checkpoints' is the folder you want to save to inside your Drive\n",
        "    drive_folder = '/content/gdrive/MyDrive/DL_Checkpoints'\n",
        "\n",
        "    # Ensure the directory exists before attempting to save\n",
        "    os.makedirs(drive_folder, exist_ok=True)\n",
        "\n",
        "    # 2. Combine the folder and the filename\n",
        "    full_path = os.path.join(drive_folder, filename)\n",
        "\n",
        "    # 3. Create the checkpoint dictionary\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "    }\n",
        "\n",
        "    # 4. Save the dictionary to the Google Drive path\n",
        "    torch.save(checkpoint, full_path)\n",
        "    print(f\"Checkpoint saved to Google Drive: {full_path} at epoch {epoch}\")\n",
        "\n",
        "\n",
        "def load_checkpoint_from_drive(model, optimizer=None, filename=\"autoencoder_checkpoint.pth\"):\n",
        "    \"\"\"\n",
        "    Loads a checkpoint from your Google Drive folder into the model and optimizer (if provided).\n",
        "    \"\"\"\n",
        "    # Define the same Google Drive folder path\n",
        "    drive_folder = '/content/gdrive/MyDrive/DL_Checkpoints'\n",
        "    full_path = os.path.join(drive_folder, filename)\n",
        "\n",
        "    # Check if the checkpoint file exists\n",
        "    if not os.path.exists(full_path):\n",
        "        raise FileNotFoundError(f\"Checkpoint file not found: {full_path}\")\n",
        "\n",
        "    # Load the checkpoint\n",
        "    checkpoint = torch.load(full_path, map_location=torch.device('cpu'))  # use cuda if available\n",
        "\n",
        "    # Restore model state\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Restore optimizer state (if provided)\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    # Extract metadata\n",
        "    epoch = checkpoint.get('epoch', 0)\n",
        "    loss = checkpoint.get('loss', None)\n",
        "\n",
        "    print(f\"Checkpoint loaded from: {full_path} (epoch {epoch})\")\n",
        "\n",
        "    return model, optimizer, epoch, loss\n"
      ],
      "metadata": {
        "id": "hDf-3nNAAn3t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}