{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxnBKWTop/1TLQFMXFGk3g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anuhya-web/DNN_final_template/blob/main/experimental_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0J41ABodZ3gn"
      },
      "outputs": [],
      "source": [
        "# @title Importing the necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from datasets.fingerprint import random\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import models\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as FT\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "import gc\n",
        "\n",
        "import textwrap\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PerceptualLoss, self).__init__()\n",
        "\n",
        "        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features\n",
        "\n",
        "        self.layers = nn.Sequential(*list(vgg[:16])).eval()\n",
        "\n",
        "        for p in self.layers.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        return nn.functional.l1_loss(self.layers(x), self.layers(y))"
      ],
      "metadata": {
        "id": "I7xgLOeIamxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def edge_loss(pred, target, device):\n",
        "    edge_x = torch.tensor([[1,0,-1],\n",
        "                            [2,0,-2],\n",
        "                            [1,0,-1]], dtype=torch.float32, device=device).view(1,1,3,3)\n",
        "\n",
        "    edge_y = edge_x.transpose(2,3)\n",
        "\n",
        "    def apply(x):\n",
        "        ex = nn.functional.conv2d(x, edge_x, padding=1, groups=3)\n",
        "        ey = nn.functional.conv2d(x, edge_y, padding=1, groups=3)\n",
        "        return torch.sqrt(ex**2 + ey**2 + 1e-6)\n",
        "\n",
        "    return nn.functional.l1_loss(apply(pred), apply(target))\n"
      ],
      "metadata": {
        "id": "-g_LyZqOaqQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setting up google drive to save checkpoints\n",
        "\n",
        "# This will prompt you to authorize Google Drive access\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "def save_checkpoint_to_drive(model, optimizer, epoch, loss, filename=\"autoencoder_checkpoint.pth\"):\n",
        "    \"\"\"\n",
        "    Saves the checkpoint directly to a specified folder in your mounted Google Drive.\n",
        "    \"\"\"\n",
        "    # 1. Define the full Google Drive path\n",
        "    # 'DL_Checkpoints' is the folder you want to save to inside your Drive\n",
        "    drive_folder = '/content/gdrive/MyDrive/DL_Checkpoints'\n",
        "\n",
        "    # Ensure the directory exists before attempting to save\n",
        "    os.makedirs(drive_folder, exist_ok=True)\n",
        "\n",
        "    # 2. Combine the folder and the filename\n",
        "    full_path = os.path.join(drive_folder, filename)\n",
        "\n",
        "    # 3. Create the checkpoint dictionary\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "    }\n",
        "\n",
        "    # 4. Save the dictionary to the Google Drive path\n",
        "    torch.save(checkpoint, full_path)\n",
        "    print(f\"Checkpoint saved to Google Drive: {full_path} at epoch {epoch}\")\n",
        "\n",
        "\n",
        "def load_checkpoint_from_drive(model, optimizer=None, filename=\"autoencoder_checkpoint.pth\"):\n",
        "    \"\"\"\n",
        "    Loads a checkpoint from your Google Drive folder into the model and optimizer (if provided).\n",
        "    \"\"\"\n",
        "    # Define the same Google Drive folder path\n",
        "    drive_folder = '/content/gdrive/MyDrive/DL_Checkpoints'\n",
        "    full_path = os.path.join(drive_folder, filename)\n",
        "\n",
        "    # Check if the checkpoint file exists\n",
        "    if not os.path.exists(full_path):\n",
        "        raise FileNotFoundError(f\"Checkpoint file not found: {full_path}\")\n",
        "\n",
        "    # Load the checkpoint\n",
        "    checkpoint = torch.load(full_path, map_location=torch.device('cpu'))  # use cuda if available\n",
        "\n",
        "    # Restore model state\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Restore optimizer state (if provided)\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    # Extract metadata\n",
        "    epoch = checkpoint.get('epoch', 0)\n",
        "    loss = checkpoint.get('loss', None)\n",
        "\n",
        "    print(f\"Checkpoint loaded from: {full_path} (epoch {epoch})\")\n",
        "\n",
        "    return model, optimizer, epoch, loss\n"
      ],
      "metadata": {
        "id": "19JFlyRnaysJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
        "l1_loss = nn.L1Loss\n",
        "mse_loss = nn.MSELoss()\n",
        "prec_loss = PerceptualLoss().to(device)"
      ],
      "metadata": {
        "id": "tJkdpC0Ba2Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Functions to load images and process data\n",
        "\n",
        "\n",
        "# This function just extracts the tags from the text, don't get distracted by it.\n",
        "# I changed this function a bit to fix some bugs\n",
        "def parse_gdi_text(text):\n",
        "    \"\"\"Parse GDI formatted text into structured data\"\"\"\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    images = []\n",
        "\n",
        "    for gdi in soup.find_all('gdi'):\n",
        "        # Debug: print what BeautifulSoup sees\n",
        "\n",
        "        # Method 1: Try to get image attribute directly\n",
        "        image_id = None\n",
        "        if gdi.attrs:\n",
        "            # Check for attributes like 'image1', 'image2', etc.\n",
        "            for attr_name, attr_value in gdi.attrs.items():\n",
        "                if 'image' in attr_name.lower():\n",
        "                    image_id = attr_name.replace('image', '')\n",
        "                    break\n",
        "\n",
        "        # Method 2: Extract from the tag string using regex\n",
        "        if not image_id:\n",
        "            tag_str = str(gdi)\n",
        "            match = re.search(r'<gdi\\s+image(\\d+)', tag_str)\n",
        "            if match:\n",
        "                image_id = match.group(1)\n",
        "\n",
        "        # Method 3: Fallback - use sequential numbering\n",
        "        if not image_id:\n",
        "            image_id = str(len(images) + 1)\n",
        "\n",
        "        content = gdi.get_text().strip()\n",
        "\n",
        "        # Extract tagged elements using BeautifulSoup directly\n",
        "        objects = [obj.get_text().strip() for obj in gdi.find_all('gdo')]\n",
        "        actions = [act.get_text().strip() for act in gdi.find_all('gda')]\n",
        "        locations = [loc.get_text().strip() for loc in gdi.find_all('gdl')]\n",
        "\n",
        "        images.append({\n",
        "            'image_id': image_id,\n",
        "            'description': content,\n",
        "            'objects': objects,\n",
        "            'actions': actions,\n",
        "            'locations': locations,\n",
        "            'raw_text': str(gdi)\n",
        "        })\n",
        "\n",
        "    return images\n",
        "\n",
        "# This is an utility function to show images.\n",
        "# Why do we need to do all this?\n",
        "def show_image(ax, image, de_normalize = False, img_mean = None, img_std = None):\n",
        "  \"\"\"\n",
        "  De-normalize the image (if necessary) and show image\n",
        "  \"\"\"\n",
        "  if de_normalize:\n",
        "    new_mean = -img_mean/img_std\n",
        "    new_std = 1/img_std\n",
        "\n",
        "    image = transforms.Normalize(\n",
        "        mean=new_mean,\n",
        "        std=new_std\n",
        "    )(image)\n",
        "  ax.imshow(image.permute(1, 2, 0))\n",
        "\n"
      ],
      "metadata": {
        "id": "yeZGicHwa9Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Loading the dataset\n",
        "train_dataset = load_dataset(\"daniel3303/StoryReasoning\", split=\"train\")\n",
        "test_dataset = load_dataset(\"daniel3303/StoryReasoning\", split=\"test\")"
      ],
      "metadata": {
        "id": "haIBcgXdbAUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Main dataset\n",
        "class SequencePredictionDataset(Dataset):\n",
        "    def __init__(self, original_dataset, tokenizer):\n",
        "        super(SequencePredictionDataset, self).__init__()\n",
        "        self.dataset = original_dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        # Potential experiments: Try other transforms!\n",
        "        self.transform = transforms.Compose([\n",
        "          transforms.Resize((60, 125)),# Reasonable size based on our previous analysis\n",
        "          transforms.ToTensor(), # HxWxC -> CxHxW\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      \"\"\"\n",
        "      Selects a 5 frame sequence from the dataset. Sets 4 for training and the last one\n",
        "      as a target.\n",
        "      \"\"\"\n",
        "      num_frames = self.dataset[idx][\"frame_count\"]\n",
        "      frames = self.dataset[idx][\"images\"]\n",
        "      self.image_attributes = parse_gdi_text(self.dataset[idx][\"story\"])\n",
        "\n",
        "      frame_tensors = []\n",
        "      description_list = []\n",
        "\n",
        "      for frame_idx in range(4):\n",
        "        image = FT.equalize(frames[frame_idx])\n",
        "        input_frame = self.transform(image)\n",
        "        frame_tensors.append(input_frame)\n",
        "\n",
        "        # Potential experiments: Try using the other attributes in your training\n",
        "        # objects = self.image_attributes[frame_idx][\"objects\"]\n",
        "        # actions = self.image_attributes[frame_idx][\"actions\"]\n",
        "        # locations = self.image_attributes[frame_idx][\"locations\"]\n",
        "\n",
        "        description = self.image_attributes[frame_idx][\"description\"]\n",
        "        # We need to return the tokens for NLP\n",
        "        input_ids =  self.tokenizer(description,\n",
        "                             return_tensors=\"pt\",\n",
        "                             padding=\"max_length\",\n",
        "                             truncation=True,\n",
        "                             max_length=120).input_ids\n",
        "\n",
        "        description_list.append(input_ids.squeeze(0))\n",
        "\n",
        "\n",
        "      image_target = FT.equalize(frames[4])\n",
        "      image_target = self.transform(image_target)\n",
        "      text_target = self.image_attributes[4][\"description\"]\n",
        "\n",
        "      target_ids = tokenizer(description,\n",
        "                             return_tensors=\"pt\",\n",
        "                             padding=\"max_length\",\n",
        "                             truncation=True,\n",
        "                             max_length=120).input_ids\n",
        "\n",
        "      sequence_tensor = torch.stack(frame_tensors)  # shape: (num_frames, C, H, W)\n",
        "      description_tensor = torch.stack(description_list) # (num_frames, max_length)\n",
        "\n",
        "      return (sequence_tensor, # Returning the image\n",
        "              description_tensor, # Returning the whole description\n",
        "              image_target, # Image target\n",
        "              target_ids) # Text target"
      ],
      "metadata": {
        "id": "xLpqt2vKbD0c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}