{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3C49ZdYJs6EX5uq5wn7eD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anuhya-web/DNN_final_template/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbB6m5qCAGwQ"
      },
      "outputs": [],
      "source": [
        "# @title Importing the necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "from datasets.fingerprint import random\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import models\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms.functional as FT\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "import gc\n",
        "\n",
        "import textwrap\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PerceptualLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PerceptualLoss, self).__init__()\n",
        "\n",
        "        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1).features\n",
        "\n",
        "        self.layers = nn.Sequential(*list(vgg[:16])).eval()\n",
        "\n",
        "        for p in self.layers.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        return nn.functional.l1_loss(self.layers(x), self.layers(y))"
      ],
      "metadata": {
        "id": "070ZXsLjAVwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def edge_loss(pred, target, device):\n",
        "    edge_x = torch.tensor([[1,0,-1],\n",
        "                            [2,0,-2],\n",
        "                            [1,0,-1]], dtype=torch.float32, device=device).view(1,1,3,3)\n",
        "\n",
        "    edge_y = edge_x.transpose(2,3)\n",
        "\n",
        "    def apply(x):\n",
        "        ex = nn.functional.conv2d(x, edge_x, padding=1, groups=3)\n",
        "        ey = nn.functional.conv2d(x, edge_y, padding=1, groups=3)\n",
        "        return torch.sqrt(ex**2 + ey**2 + 1e-6)\n",
        "\n",
        "    return nn.functional.l1_loss(apply(pred), apply(target))\n"
      ],
      "metadata": {
        "id": "s141JOk7AibW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setting up google drive to save checkpoints\n",
        "\n",
        "# This will prompt you to authorize Google Drive access\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "def save_checkpoint_to_drive(model, optimizer, epoch, loss, filename=\"autoencoder_checkpoint.pth\"):\n",
        "    \"\"\"\n",
        "    Saves the checkpoint directly to a specified folder in your mounted Google Drive.\n",
        "    \"\"\"\n",
        "    # 1. Define the full Google Drive path\n",
        "    # 'DL_Checkpoints' is the folder you want to save to inside your Drive\n",
        "    drive_folder = '/content/gdrive/MyDrive/DL_Checkpoints'\n",
        "\n",
        "    # Ensure the directory exists before attempting to save\n",
        "    os.makedirs(drive_folder, exist_ok=True)\n",
        "\n",
        "    # 2. Combine the folder and the filename\n",
        "    full_path = os.path.join(drive_folder, filename)\n",
        "\n",
        "    # 3. Create the checkpoint dictionary\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "    }\n",
        "\n",
        "    # 4. Save the dictionary to the Google Drive path\n",
        "    torch.save(checkpoint, full_path)\n",
        "    print(f\"Checkpoint saved to Google Drive: {full_path} at epoch {epoch}\")\n",
        "\n",
        "\n",
        "def load_checkpoint_from_drive(model, optimizer=None, filename=\"autoencoder_checkpoint.pth\"):\n",
        "    \"\"\"\n",
        "    Loads a checkpoint from your Google Drive folder into the model and optimizer (if provided).\n",
        "    \"\"\"\n",
        "    # Define the same Google Drive folder path\n",
        "    drive_folder = '/content/gdrive/MyDrive/DL_Checkpoints'\n",
        "    full_path = os.path.join(drive_folder, filename)\n",
        "\n",
        "    # Check if the checkpoint file exists\n",
        "    if not os.path.exists(full_path):\n",
        "        raise FileNotFoundError(f\"Checkpoint file not found: {full_path}\")\n",
        "\n",
        "    # Load the checkpoint\n",
        "    checkpoint = torch.load(full_path, map_location=torch.device('cpu'))  # use cuda if available\n",
        "\n",
        "    # Restore model state\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Restore optimizer state (if provided)\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    # Extract metadata\n",
        "    epoch = checkpoint.get('epoch', 0)\n",
        "    loss = checkpoint.get('loss', None)\n",
        "\n",
        "    print(f\"Checkpoint loaded from: {full_path} (epoch {epoch})\")\n",
        "\n",
        "    return model, optimizer, epoch, loss\n"
      ],
      "metadata": {
        "id": "hDf-3nNAAn3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
        "l1_loss = nn.L1Loss\n",
        "mse_loss = nn.MSELoss()\n",
        "prec_loss = PerceptualLoss().to(device)"
      ],
      "metadata": {
        "id": "XZqLJpcVA1cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Functions to load images and process data\n",
        "\n",
        "\n",
        "# This function just extracts the tags from the text, don't get distracted by it.\n",
        "# I changed this function a bit to fix some bugs\n",
        "def parse_gdi_text(text):\n",
        "    \"\"\"Parse GDI formatted text into structured data\"\"\"\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    images = []\n",
        "\n",
        "    for gdi in soup.find_all('gdi'):\n",
        "        # Debug: print what BeautifulSoup sees\n",
        "\n",
        "        # Method 1: Try to get image attribute directly\n",
        "        image_id = None\n",
        "        if gdi.attrs:\n",
        "            # Check for attributes like 'image1', 'image2', etc.\n",
        "            for attr_name, attr_value in gdi.attrs.items():\n",
        "                if 'image' in attr_name.lower():\n",
        "                    image_id = attr_name.replace('image', '')\n",
        "                    break\n",
        "\n",
        "        # Method 2: Extract from the tag string using regex\n",
        "        if not image_id:\n",
        "            tag_str = str(gdi)\n",
        "            match = re.search(r'<gdi\\s+image(\\d+)', tag_str)\n",
        "            if match:\n",
        "                image_id = match.group(1)\n",
        "\n",
        "        # Method 3: Fallback - use sequential numbering\n",
        "        if not image_id:\n",
        "            image_id = str(len(images) + 1)\n",
        "\n",
        "        content = gdi.get_text().strip()\n",
        "\n",
        "        # Extract tagged elements using BeautifulSoup directly\n",
        "        objects = [obj.get_text().strip() for obj in gdi.find_all('gdo')]\n",
        "        actions = [act.get_text().strip() for act in gdi.find_all('gda')]\n",
        "        locations = [loc.get_text().strip() for loc in gdi.find_all('gdl')]\n",
        "\n",
        "        images.append({\n",
        "            'image_id': image_id,\n",
        "            'description': content,\n",
        "            'objects': objects,\n",
        "            'actions': actions,\n",
        "            'locations': locations,\n",
        "            'raw_text': str(gdi)\n",
        "        })\n",
        "\n",
        "    return images\n",
        "\n",
        "# This is an utility function to show images.\n",
        "# Why do we need to do all this?\n",
        "def show_image(ax, image, de_normalize = False, img_mean = None, img_std = None):\n",
        "  \"\"\"\n",
        "  De-normalize the image (if necessary) and show image\n",
        "  \"\"\"\n",
        "  if de_normalize:\n",
        "    new_mean = -img_mean/img_std\n",
        "    new_std = 1/img_std\n",
        "\n",
        "    image = transforms.Normalize(\n",
        "        mean=new_mean,\n",
        "        std=new_std\n",
        "    )(image)\n",
        "  ax.imshow(image.permute(1, 2, 0))\n",
        "\n"
      ],
      "metadata": {
        "id": "Fn6TdcxsA126"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Loading the dataset\n",
        "train_dataset = load_dataset(\"daniel3303/StoryReasoning\", split=\"train\")\n",
        "test_dataset = load_dataset(\"daniel3303/StoryReasoning\", split=\"test\")"
      ],
      "metadata": {
        "id": "P764JGPSA2MN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Main dataset\n",
        "class SequencePredictionDataset(Dataset):\n",
        "    def __init__(self, original_dataset, tokenizer):\n",
        "        super(SequencePredictionDataset, self).__init__()\n",
        "        self.dataset = original_dataset\n",
        "        self.tokenizer = tokenizer\n",
        "        # Potential experiments: Try other transforms!\n",
        "        self.transform = transforms.Compose([\n",
        "          transforms.Resize((60, 125)),# Reasonable size based on our previous analysis\n",
        "          transforms.ToTensor(), # HxWxC -> CxHxW\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      \"\"\"\n",
        "      Selects a 5 frame sequence from the dataset. Sets 4 for training and the last one\n",
        "      as a target.\n",
        "      \"\"\"\n",
        "      num_frames = self.dataset[idx][\"frame_count\"]\n",
        "      frames = self.dataset[idx][\"images\"]\n",
        "      self.image_attributes = parse_gdi_text(self.dataset[idx][\"story\"])\n",
        "\n",
        "      frame_tensors = []\n",
        "      description_list = []\n",
        "\n",
        "      for frame_idx in range(4):\n",
        "        image = FT.equalize(frames[frame_idx])\n",
        "        input_frame = self.transform(image)\n",
        "        frame_tensors.append(input_frame)\n",
        "\n",
        "        # Potential experiments: Try using the other attributes in your training\n",
        "        # objects = self.image_attributes[frame_idx][\"objects\"]\n",
        "        # actions = self.image_attributes[frame_idx][\"actions\"]\n",
        "        # locations = self.image_attributes[frame_idx][\"locations\"]\n",
        "\n",
        "        description = self.image_attributes[frame_idx][\"description\"]\n",
        "        # We need to return the tokens for NLP\n",
        "        input_ids =  self.tokenizer(description,\n",
        "                             return_tensors=\"pt\",\n",
        "                             padding=\"max_length\",\n",
        "                             truncation=True,\n",
        "                             max_length=120).input_ids\n",
        "\n",
        "        description_list.append(input_ids.squeeze(0))\n",
        "\n",
        "\n",
        "      image_target = FT.equalize(frames[4])\n",
        "      image_target = self.transform(image_target)\n",
        "      text_target = self.image_attributes[4][\"description\"]\n",
        "\n",
        "      target_ids = tokenizer(description,\n",
        "                             return_tensors=\"pt\",\n",
        "                             padding=\"max_length\",\n",
        "                             truncation=True,\n",
        "                             max_length=120).input_ids\n",
        "\n",
        "      sequence_tensor = torch.stack(frame_tensors)  # shape: (num_frames, C, H, W)\n",
        "      description_tensor = torch.stack(description_list) # (num_frames, max_length)\n",
        "\n",
        "      return (sequence_tensor, # Returning the image\n",
        "              description_tensor, # Returning the whole description\n",
        "              image_target, # Image target\n",
        "              target_ids) # Text target"
      ],
      "metadata": {
        "id": "vsSSGBohA-5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Text task dataset (text autoencoding)\n",
        "class TextTaskDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      num_frames = self.dataset[idx][\"frame_count\"]\n",
        "      self.image_attributes = parse_gdi_text(self.dataset[idx][\"story\"])\n",
        "\n",
        "      # Pick\n",
        "      frame_idx = np.random.randint(0, 5)\n",
        "      description = self.image_attributes[frame_idx][\"description\"]\n",
        "\n",
        "      return description  # Returning the whole description\n"
      ],
      "metadata": {
        "id": "cFoIJEDsA_Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dataset for image autoencoder task\n",
        "class AutoEncoderTaskDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transforms.Compose([\n",
        "          transforms.Resize((240, 500)),# Reasonable size based on our previous analysis\n",
        "          transforms.ToTensor(), # HxWxC -> CxHxW\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      num_frames = self.dataset[idx][\"frame_count\"]\n",
        "      frames = self.dataset[idx][\"images\"]\n",
        "\n",
        "      # Pick a frame at random\n",
        "      frame_idx = torch.randint(0, num_frames-1, (1,)).item()\n",
        "      input_frame = self.transform(frames[frame_idx]) # Input to the autoencoder\n",
        "\n",
        "      return input_frame, # Returning the image"
      ],
      "metadata": {
        "id": "wfFn0Li_BO8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title For the Sequence prediction task\n",
        "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\",  padding=True, truncation=True)\n",
        "sp_train_dataset = SequencePredictionDataset(train_dataset, tokenizer) # Instantiate the train dataset\n",
        "sp_test_dataset = SequencePredictionDataset(test_dataset, tokenizer) # Instantiate the test dataset\n",
        "\n",
        "# Let's do things properly, we will also have a validation split\n",
        "# Split the training dataset into training and validation sets\n",
        "train_size = int(0.8 * len(sp_train_dataset))\n",
        "val_size = len(sp_train_dataset) - train_size\n",
        "train_subset, val_subset = random_split(sp_train_dataset, [train_size, val_size])\n",
        "\n",
        "# Instantiate the dataloaders\n",
        "train_dataloader = DataLoader(train_subset, batch_size=8, shuffle=True)\n",
        "# We will use the validation set to visualize the progress.\n",
        "val_dataloader = DataLoader(val_subset, batch_size=4, shuffle=True)\n",
        "test_dataloader = DataLoader(sp_test_dataset, batch_size=4, shuffle=False)\n"
      ],
      "metadata": {
        "id": "27PZyBePBPTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title For the text task\n",
        "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\",  padding=True, truncation=True)\n",
        "text_dataset = TextTaskDataset(train_dataset)\n",
        "text_dataloader = DataLoader(text_dataset, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "xPayySECBWoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title For the image autoencoder task\n",
        "autoencoder_dataset = AutoEncoderTaskDataset(train_dataset)\n",
        "autoencoder_dataloader = DataLoader(autoencoder_dataset, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "zI_dCDCuBXBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Testing some of the outputs of the SP dataset\n",
        "frames, descriptions, image_target, text_target = sp_train_dataset[np.random.randint(0,400)]\n",
        "\n",
        "print(\"Description: \", descriptions.shape)\n",
        "figure, ax = plt.subplots(1,1)\n",
        "show_image(ax, image_target)\n",
        "\n",
        "# Do some tests on the batches (try with batch size small)\n",
        "frames, descriptions, image_target, text_target = next(iter(train_dataloader))\n",
        "print(frames.shape)\n",
        "print(descriptions.shape)"
      ],
      "metadata": {
        "id": "xoZyxZGYBoz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title The text autoencoder (Seq2Seq)\n",
        "\n",
        "class EncoderLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "      Encodes a sequence of tokens into a latent space representation.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,\n",
        "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        embedded = self.embedding(input_seq)\n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        return outputs, hidden, cell\n",
        "\n",
        "class DecoderLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "      Decodes a latent space representation into a sequence of tokens.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers,\n",
        "                            batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
        "        self.out = nn.Linear(hidden_dim, vocab_size) # Should be hidden_dim\n",
        "\n",
        "    def forward(self, input_seq, hidden, cell):\n",
        "        embedded = self.embedding(input_seq)\n",
        "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "        prediction = self.out(output)\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "# We create the basic text autoencoder (a special case of a sequence to sequence model)\n",
        "class Seq2SeqLSTM(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, target_seq):\n",
        "        # input_seq and target_seq are both your 'input_ids'\n",
        "        # 1. Encode the input sequence\n",
        "        _enc_out, hidden, cell = self.encoder(input_seq)\n",
        "\n",
        "        # 2. Create the \"shifted\" decoder input for teacher forcing.\n",
        "        # We want to predict target_seq[:, 1:]\n",
        "        # So, we feed in target_seq[:, :-1]\n",
        "        # (i.e., feed \"[SOS], hello, world\" to predict \"hello, world, [EOS]\")\n",
        "        decoder_input = target_seq[:, :-1]\n",
        "\n",
        "        # 3. Run the decoder *once* on the entire sequence.\n",
        "        # It takes the encoder's final state (hidden, cell)\n",
        "        # and the full \"teacher\" sequence (decoder_input).\n",
        "        predictions, _hidden, _cell = self.decoder(decoder_input, hidden, cell)\n",
        "\n",
        "        # predictions shape will be (batch_size, seq_len-1, vocab_size)\n",
        "        return predictions\n"
      ],
      "metadata": {
        "id": "Yh_dGRiWBquJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Utility functions for NLP tasks\n",
        "def generate(model, hidden, cell, max_len, sos_token_id, eos_token_id):\n",
        "      \"\"\"\n",
        "        This function generates a sequence of tokens using the provided decoder.\n",
        "      \"\"\"\n",
        "      # Ensure the model is in evaluation mode\n",
        "      model.eval()\n",
        "\n",
        "      # 2. SETUP DECODER INPUT\n",
        "      # Start with the SOS token, shape (1, 1)\n",
        "      dec_input = torch.tensor([[sos_token_id]], dtype=torch.long, device=device)\n",
        "      # hidden = torch.zeros(1, 1, hidden_dim, device=device)\n",
        "      # cell = torch.zeros(1, 1, hidden_dim, device=device)\n",
        "\n",
        "      generated_tokens = []\n",
        "\n",
        "      # 3. AUTOREGRESSIVE LOOP\n",
        "      for _ in range(max_len):\n",
        "          with torch.no_grad():\n",
        "              # Run the decoder one step at a time\n",
        "              # dec_input is (1, 1) hereâ€”it's just the last predicted token\n",
        "              prediction, hidden, cell = model(dec_input, hidden, cell)\n",
        "\n",
        "          logits = prediction.squeeze(1) # Shape (1, vocab_size)\n",
        "          temperature = 0.9 # <--- Try a value between 0.5 and 1.0\n",
        "\n",
        "          # 1. Divide logits by temperature\n",
        "          # 2. Apply softmax to get probabilities\n",
        "          # 3. Use multinomial to sample one token based on the probabilities\n",
        "          probabilities = torch.softmax(logits / temperature, dim=-1)\n",
        "          next_token = torch.multinomial(probabilities, num_samples=1)\n",
        "\n",
        "          token_id = next_token.squeeze().item()\n",
        "\n",
        "          # Check for the End-of-Sequence token\n",
        "          if token_id == eos_token_id:\n",
        "              break\n",
        "\n",
        "          if token_id == 0 or token_id == sos_token_id:\n",
        "              continue\n",
        "\n",
        "            # Append the predicted token\n",
        "          generated_tokens.append(token_id)\n",
        "\n",
        "          # The predicted token becomes the input for the next iteration\n",
        "          dec_input = next_token\n",
        "\n",
        "      # Return the list of generated token IDs\n",
        "      return generated_tokens\n"
      ],
      "metadata": {
        "id": "Gj5BbnMeBrGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Do some tests\n",
        "# desc = text_dataset[np.random.randint(0, 100)]\n",
        "# print(f\"Input: {desc}\")\n",
        "# input_ids = tokenizer(desc, return_tensors=\"pt\",  padding=True, truncation=True).input_ids\n",
        "# input_ids = input_ids.to(device)\n",
        "# generated_tokens = generate(model, hidden, cell, max_len=100, sos_token_id=tokenizer.cls_token_id, eos_token_id=tokenizer.sep_token_id)\n",
        "# print(\"Output: \", tokenizer.decode(generated_tokens))"
      ],
      "metadata": {
        "id": "Q5GW1vbYB0jt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title The visual autoencoder\n",
        "class Backbone(nn.Module):\n",
        "    \"\"\"\n",
        "      Main convolutional blocks for our CNN\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim=16, output_w = 8, output_h = 16):\n",
        "        super(Backbone, self).__init__()\n",
        "        # Encoder convolutional layers\n",
        "        self.encoder_conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 7, stride=2, padding=3),\n",
        "            nn.GroupNorm(8, 16),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            nn.Conv2d(16, 32, 5, stride=2, padding=2),\n",
        "            nn.GroupNorm(8, 32),\n",
        "            nn.LeakyReLU(0.1),\n",
        "\n",
        "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
        "            nn.GroupNorm(8, 64),\n",
        "            nn.LeakyReLU(0.1),\n",
        "        )\n",
        "\n",
        "        # Calculate flattened dimension for linear layer\n",
        "        self.flatten_dim = 64 * output_w * output_h\n",
        "        # Latent space layers\n",
        "        self.fc1 = nn.Sequential(nn.Linear(self.flatten_dim, latent_dim), nn.ReLU())\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder_conv(x)\n",
        "        x = x.view(-1, self.flatten_dim)  # flatten for linear layer\n",
        "        z = self.fc1(x)\n",
        "        return z\n",
        "\n",
        "class VisualEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "      Encodes an image into a latent space representation. Note the two pathways\n",
        "      to try to disentangle the mean pattern from the image\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim=16, output_w = 8, output_h = 16):\n",
        "        super(VisualEncoder, self).__init__()\n",
        "\n",
        "        self.context_backbone = Backbone(latent_dim, output_w, output_h)\n",
        "        self.content_backbone = Backbone(latent_dim, output_w, output_h)\n",
        "\n",
        "        self.projection = nn.Linear(2*latent_dim, latent_dim)\n",
        "    def forward(self, x):\n",
        "        z_context = self.context_backbone(x)\n",
        "        z_content = self.content_backbone(x)\n",
        "        z = torch.cat((z_content, z_context), dim=1)\n",
        "        z = self.projection(z)\n",
        "        return z\n",
        "\n",
        "class VisualDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "      Decodes a latent representation into a content image and a context image\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim=16, output_w = 8, output_h = 16):\n",
        "        super(VisualDecoder, self).__init__()\n",
        "        self.imh = 60\n",
        "        self.imw = 125\n",
        "        self.flatten_dim = 64 * output_w * output_h\n",
        "        self.output_w = output_w\n",
        "        self.output_h = output_h\n",
        "\n",
        "        self.fc1 = nn.Linear(latent_dim, self.flatten_dim)\n",
        "\n",
        "        self.decoder_conv = nn.Sequential(\n",
        "          nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=(1,1)),\n",
        "          nn.GroupNorm(8, 32),\n",
        "          nn.LeakyReLU(0.1),\n",
        "\n",
        "          nn.ConvTranspose2d(32, 16, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
        "          nn.GroupNorm(8, 16),\n",
        "          nn.LeakyReLU(0.1),\n",
        "\n",
        "          nn.ConvTranspose2d(16, 3, kernel_size=7, stride=2, padding=3, output_padding=(1, 1)),\n",
        "          nn.Sigmoid() # Use nn.Tanh() if your data is normalized to [-1, 1]\n",
        "      )\n",
        "\n",
        "    def forward(self, z):\n",
        "      x = self.fc1(z)\n",
        "\n",
        "      x_content = self.decode_image(x)\n",
        "      x_context = self.decode_image(x)\n",
        "\n",
        "      return x_content, x_context\n",
        "\n",
        "    def decode_image(self, x):\n",
        "      x = x.view(-1, 64, self.output_w, self.output_h)      # reshape to conv feature map\n",
        "      x = self.decoder_conv(x)\n",
        "      x = x[:, :, :self.imh, :self.imw]          # crop to original size if needed\n",
        "      return x\n",
        "\n",
        "class VisualAutoencoder( nn.Module):\n",
        "    def __init__(self, latent_dim=16, output_w = 8, output_h = 16):\n",
        "        super(VisualAutoencoder, self).__init__()\n",
        "        self.encoder = VisualEncoder(latent_dim, output_w, output_h)\n",
        "        self.decoder = VisualDecoder(latent_dim, output_w, output_h)\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, channels, attn_temperature=1.0, attn_gamma=1.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attn_temperature = attn_temperature\n",
        "        self.attn_gamma = attn_gamma\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(channels, channels // 2, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(channels // 2, 1, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.conv(x)\n",
        "        logits = logits * self.attn_temperature\n",
        "        logits = torch.exp(logits)\n",
        "\n",
        "        attn = self.activation(logits)\n",
        "        attn = attn / (attn.sum(dim=1, keepdim=True) + 1e-5)\n",
        "        attn = attn ** self.attn_gamma\n",
        "\n",
        "        return attn\n",
        "\n",
        "        # return x * logits\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        x_hat = self.decoder(z)\n",
        "        return x_hat\n",
        "\n"
      ],
      "metadata": {
        "id": "2b5nKOJEB3_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CrossModalFusion(nn.Module):\n",
        "    def __init__(self, image_dim, text_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.image_attn = nn.Linear(image_dim, hidden_dim)\n",
        "        self.text_attn = nn.Linear(text_dim, hidden_dim)\n",
        "\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            hidden_dim,\n",
        "            num_heads=8,\n",
        "            dropout=0.1\n",
        "        )\n",
        "\n",
        "        self.out = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, image_features, text_features):\n",
        "        image_attn = self.image_attn(image_features)\n",
        "        text_attn = self.text_attn(text_features)\n",
        "\n",
        "        attn_output, _ = self.attn(image_attn, text_attn, text_attn)\n",
        "        attn_output = self.out(attn_output)\n",
        "\n",
        "        return attn_output\n"
      ],
      "metadata": {
        "id": "AP86duHxB-Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title A simple attention architecture\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        # This \"attention\" layer learns a query vector\n",
        "        self.attn = nn.Linear(hidden_dim, 1)\n",
        "        self.softmax = nn.Softmax(dim=1) # Over the sequence length\n",
        "\n",
        "    def forward(self, rnn_outputs):\n",
        "        # rnn_outputs shape: [batch, seq_len, hidden_dim]\n",
        "\n",
        "        # Pass through linear layer to get \"energy\" scores\n",
        "        energy = self.attn(rnn_outputs).squeeze(2) # Shape: [batch, seq_len]\n",
        "\n",
        "        # Get attention weights\n",
        "        attn_weights = self.softmax(energy) # Shape: [batch, seq_len]\n",
        "\n",
        "        # Apply weights\n",
        "        # attn_weights.unsqueeze(1) -> [batch, 1, seq_len]\n",
        "        # bmm with rnn_outputs -> [batch, 1, hidden_dim]\n",
        "        context = torch.bmm(attn_weights.unsqueeze(1), rnn_outputs)\n",
        "\n",
        "        # Squeeze to get final context vector\n",
        "        return context.squeeze(1) # Shape: [batch, hidden_dim]"
      ],
      "metadata": {
        "id": "8lC3MjX8CI6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title The main sequence predictor model\n",
        "\n",
        "class SequencePredictor(nn.Module):\n",
        "    def __init__(self, visual_autoencoder, text_autoencoder, latent_dim,\n",
        "                 gru_hidden_dim):\n",
        "        super(SequencePredictor, self).__init__()\n",
        "\n",
        "        # --- 1. Static Encoders ---\n",
        "        # (These process one pair at a time)\n",
        "        self.image_encoder = visual_autoencoder.encoder\n",
        "        self.text_encoder = text_autoencoder.encoder\n",
        "\n",
        "        # --- 2. Temporal Encoder ---\n",
        "        # (This processes the sequence of pairs)\n",
        "        fusion_dim = latent_dim * 2 # z_visual + z_text\n",
        "        self.temporal_rnn = nn.GRU(fusion_dim, latent_dim, batch_first=True)\n",
        "\n",
        "        # --- 3. Attention ---\n",
        "        self.attention = Attention(gru_hidden_dim)\n",
        "\n",
        "        # --- 4. Final Projection ---\n",
        "        # cat(h, context) -> gru_hidden_dim * 2\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(gru_hidden_dim * 2, latent_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # --- 5. Decoders ---\n",
        "        # (These predict the *next* item)\n",
        "        self.image_decoder = visual_autoencoder.decoder\n",
        "        self.text_decoder = text_autoencoder.decoder\n",
        "\n",
        "        self.fused_to_h0 = nn.Linear(latent_dim, 16)\n",
        "        self.fused_to_c0 = nn.Linear(latent_dim, 16)\n",
        "\n",
        "    def forward(self, image_seq, text_seq, target_seq):\n",
        "        # image_seq shape: [batch, seq_len, C, H, W]\n",
        "        # text_seq shape:  [batch, seq_len, text_len]\n",
        "        # target_text_for_teacher_forcing: [batch, text_len] (This is the last text)\n",
        "\n",
        "        batch_size, seq_len, C, H, W = image_seq.shape\n",
        "\n",
        "        # --- 1 & 2: Run Static Encoders over the sequence ---\n",
        "        # We can't pass a 5D/4D tensor to the encoders.\n",
        "        # We \"flatten\" the batch and sequence dimensions.\n",
        "\n",
        "        # Reshape for image_encoder\n",
        "        img_flat = image_seq.view(batch_size * seq_len, C, H, W)\n",
        "        # Reshape for text_encoder\n",
        "        txt_flat = text_seq.view(batch_size * seq_len, -1) # -1 infers text_len\n",
        "\n",
        "        # Run encoders\n",
        "        z_v_flat = self.image_encoder(img_flat) # Shape: [b*s, latent]\n",
        "        _, hidden, cell = self.text_encoder(txt_flat) # Shape: [b*s, latent]\n",
        "\n",
        "        # Combine\n",
        "        z_fusion_flat = torch.cat((z_v_flat, hidden.squeeze(0)), dim=1) # Shape: [b*s, fusion_dim]\n",
        "\n",
        "        # \"Un-flatten\" back into a sequence\n",
        "        z_fusion_seq = z_fusion_flat.view(batch_size, seq_len, -1) # Shape: [b, s, fusion_dim]\n",
        "\n",
        "        # --- 3. Run Temporal Encoder ---\n",
        "        # zseq shape: [b, s, gru_hidden]\n",
        "        # h    shape: [1, b, gru_hidden]\n",
        "        zseq, h = self.temporal_rnn(z_fusion_seq)\n",
        "        h = h.squeeze(0) # Shape: [b, gru_hidden]\n",
        "\n",
        "        # --- 4. Attention ---\n",
        "        context = self.attention(zseq) # Shape: [b, gru_hidden]\n",
        "\n",
        "        # --- 5. Final Prediction Vector (z) ---\n",
        "        z = self.projection(torch.cat((h, context), dim=1)) # Shape: [b, joint_latent_dim]\n",
        "\n",
        "        # --- 6. Decode (Predict pk) ---\n",
        "        pred_image_content, pred_image_context = self.image_decoder(z)\n",
        "\n",
        "        h0 = self.fused_to_h0(z).unsqueeze(0)\n",
        "        c0 = self.fused_to_c0(z).unsqueeze(0)\n",
        "\n",
        "        decoder_input = target_seq[:, :,:-1].squeeze(1)\n",
        "\n",
        "        # 3. Run the decoder *once* on the entire sequence.\n",
        "        # It takes the encoder's final state (hidden, cell)\n",
        "        # and the full \"teacher\" sequence (decoder_input).\n",
        "        predicted_text_logits_k, _hidden, _cell = self.text_decoder(decoder_input, h0, c0)\n",
        "\n",
        "        return pred_image_content, pred_image_context, predicted_text_logits_k,h0, c0"
      ],
      "metadata": {
        "id": "XlZVrgPCCLt4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}